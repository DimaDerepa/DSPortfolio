{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "from typing import Optional, Any\n",
    "from pydantic import BaseModel, ValidationError, validator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Structure Definition\n",
    "class DatasetStructure(BaseModel):\n",
    "    name: str\n",
    "    content: str\n",
    "    attributes: str\n",
    "    category_a: int|str\n",
    "    category_b: int|str\n",
    "    category_c: int|str\n",
    "    value_1: int|str\n",
    "    value_2: int|str\n",
    "    metric_1: float|str\n",
    "    metric_2: float|str\n",
    "    metric_3: float|str\n",
    "    classification: Optional[str] = \"\"\n",
    "\n",
    "    @validator('name', 'content', 'attributes')\n",
    "    @classmethod\n",
    "    def process_text(cls, value):\n",
    "        pattern = r\"\\n+|\\t+\"\n",
    "        value = re.sub(pattern, \"\", value)\n",
    "        value = re.sub(r\"\\;\", \",\", value)\n",
    "        return value\n",
    "\n",
    "    @validator('category_a', 'category_b', 'category_c', 'value_1', 'value_2', 'metric_1', 'metric_2', 'metric_3', pre=True)\n",
    "    @classmethod\n",
    "    def process_numeric(cls, value):\n",
    "        if not isinstance(value, float):\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except:\n",
    "                value = \"Unknown\"\n",
    "        if value == 0.0:\n",
    "            return \"Unknown\"\n",
    "        return value\n",
    "\n",
    "def compose_data_object(data_object: DatasetStructure) -> str:\n",
    "    return f\"\"\"###NAME: {data_object.name}</s>###CONTENT: {data_object.content}</s>###ATTRIBUTES: {data_object.attributes}</s>###VALUE1: {data_object.value_1}</s>###VALUE2: {data_object.value_2}</s>###CATA: {data_object.category_a}</s>###CATB: {data_object.category_b}</s>###CATC: {data_object.category_c}</s>###METRIC1: {data_object.metric_1}</s>###METRIC2: {data_object.metric_2}</s>###METRIC3: {data_object.metric_3}</s>\"\"\"\n",
    "\n",
    "# Model Configuration\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Training Configuration\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model_name=\"intfloat/multilingual-e5-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def prepare_model(self, num_labels):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(self, train_dataset, test_dataset, output_dir=\"./private_results\"):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=8,\n",
    "            per_device_train_batch_size=14,\n",
    "            per_device_eval_batch_size=8,\n",
    "            weight_decay=0.001,\n",
    "            logging_dir='./private_logs',\n",
    "            load_best_model_at_end=True,\n",
    "            learning_rate=1e-5,\n",
    "            evaluation_strategy='epoch',\n",
    "            logging_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            save_total_limit=1\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "\n",
    "        return trainer.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "        return {'F1': f1}\n",
    "\n",
    "    def inference(self, text: str, int_to_label_map: dict) -> str:\n",
    "        inputs = self.tokenizer(text, padding=False, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "        output = self.model(**inputs)\n",
    "        predicted_class = output.logits[0].tolist().index(sorted(output.logits[0], reverse=True)[0])\n",
    "        return int_to_label_map[predicted_class]\n",
    "\n",
    "# Usage Example:\n",
    "def main():\n",
    "    seed_all(3258976)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    private_data = pd.DataFrame()  # Your data loading logic here\n",
    "    \n",
    "    # Prepare labels\n",
    "    unique_labels = private_data['classification'].unique()\n",
    "    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "    int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "    \n",
    "    # Process data\n",
    "    X = [compose_data_object(DatasetStructure(**obj)) for obj in private_data.to_dict(orient=\"records\")]\n",
    "    y = private_data['classification'].map(label_to_int).values.tolist()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3258976)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ModelTrainer()\n",
    "    trainer.prepare_model(num_labels=len(unique_labels))\n",
    "    \n",
    "    # Train model\n",
    "    results = trainer.train(X_train, X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = [trainer.inference(text, int_to_label) for text in X_test]\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
